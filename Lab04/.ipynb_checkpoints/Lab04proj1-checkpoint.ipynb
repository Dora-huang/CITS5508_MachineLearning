{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Lab04 sheet </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID: 22670287** <br>\n",
    "**Student Name: Xiaoyan Huang**<br>\n",
    "**Date created: 21/04/2020**<br>\n",
    "**Last modified: 28/04/2020**<br>\n",
    "\n",
    "The main goal of Project1 is to compare the performances in AdaBoost Regressor and a Gradient Boosting Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whtwine = pd.read_csv(\"winequality-white.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whtwine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = whtwine.iloc[:,:11]\n",
    "y = whtwine['quality']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_mean = X_train.mean(axis=0)\n",
    "xlab = list(X.columns)\n",
    "ss = np.arange(len(xlab))\n",
    "plt.bar(ss, X_mean)\n",
    "plt.xticks(ss, xlab, rotation = 45)\n",
    "plt.ylabel('Mean')\n",
    "plt.title('the mean of each feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = preprocessing.scale(X_train)\n",
    "X_test_sc = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc.mean(axis=0), X_train_sc.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The whitewine data was split into training set and test set by a 85/15 proportion. There are 4163 instances in training set and the rest of instances in test set.\n",
    "\n",
    "- After the inspection of data via `describe` function or the bar chart, the ranges of features are different. It is necessary to scale the data both in training and test set. The new variables *X_train_sc* and *X_test_sc* are scaled to have a mean of zero and a standard deviations of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "svr_clf = SVR(kernel=\"rbf\")\n",
    "ada_clf = AdaBoostRegressor(\n",
    "    base_estimator=svr_clf,\n",
    "    n_estimators=6, learning_rate=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_value_test = ada_clf.predict(X_test_sc)\n",
    "prd_value_train = ada_clf.predict(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_prd_value_test = np.around(prd_value_test)\n",
    "int_prd_value_train = np.around(prd_value_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is AdaBoost Regressor using SVM regressor with RBF kernel and it has only 6 estimators. However, the predictive values aren't integers due to the regressor model. Thus, using `np.around` converts output into integer for both training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inspect Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_test = mean_squared_error(y_test,int_prd_value_test)\n",
    "print(\"Mean square error of final model in test data is %s \" % mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_value_irt = ada_clf.staged_predict(X_test_sc)\n",
    "mes_irt = []\n",
    "for i in prd_value_irt:\n",
    "    int_prd_value_irt = np.around(i)\n",
    "    mes_irt.append(mean_squared_error(y_test, int_prd_value_irt))\n",
    "print(\"The MSEs of all the models in test set \\n\")\n",
    "print(mes_irt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = mean_squared_error(y_train, int_prd_value_train)\n",
    "print(\"Mean square error of final model in training data is %s \" % mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_value_irt2 = ada_clf.staged_predict(X_train_sc)\n",
    "mes_irt2 = []\n",
    "for i in prd_value_irt2:\n",
    "    int_prd_value_irt2 = np.around(i)\n",
    "    mes_irt2.append(mean_squared_error(y_train, int_prd_value_irt2))\n",
    "print(\"The MSEs of all the models in training set \\n\")\n",
    "print(mes_irt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [1,2,3,4,5,6]\n",
    "plt.plot(x, mes_irt, marker=\"o\", c='r', label=\"MSE of test\")\n",
    "plt.plot(x, mes_irt2, marker=\"*\", c='b', label=\"MSE of train\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iteration time\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared the mean square error of final models on traning set with test set, the fact is that the Mean Square Error of training data is lower than test data (0.45 versus 0.58). \n",
    "- And then, the MSEs of all the intermediate models are calculated via `staged_predict` function on both datasets, and these results are exhibited by a line chart. From the above plot, we know that the MSEs are both descreasing as iteration time increases and these decrease significantly when iteration time is equal to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2  Inspect Raw Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ada_test = pd.DataFrame()\n",
    "errors_ada_test['predictive'] = int_prd_value_test\n",
    "errors_ada_test['y_test'] = y_test.tolist()\n",
    "errors_ada_test['errors'] = errors_ada_test['predictive'] - errors_ada_test['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_test = errors_ada_test['errors'].value_counts()\n",
    "cnt_test = cnt_test.sort_index(ascending=True)\n",
    "cnt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab = cnt_test.index\n",
    "ss = np.arange(len(xlab))\n",
    "plt.bar(ss, cnt_test)\n",
    "plt.xticks(ss, xlab)\n",
    "plt.ylabel('Count')\n",
    "plt.title('the raw errors of the final model for Adaboost on test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ada_train = pd.DataFrame()\n",
    "errors_ada_train['predictive'] = int_prd_value_train\n",
    "errors_ada_train['y_train'] = y_train.tolist()\n",
    "errors_ada_train['errors'] = errors_ada_train['predictive'] - errors_ada_train['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_train = errors_ada_train['errors'].value_counts()\n",
    "cnt_train = cnt_train.sort_index(ascending=True)\n",
    "cnt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab = cnt_test.index\n",
    "ss = np.arange(len(xlab))\n",
    "plt.bar(ss, cnt_train)\n",
    "plt.xticks(ss, xlab)\n",
    "plt.ylabel('Count')\n",
    "plt.title('the raw errors of the final model for Adaboost on train set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to inspect raw errors, the *errors_ada_test* and *errors_ada_train* dataframe are created to store the predictive value, ground true and raw errors. And then, these are counted and exhibited by bar charts.\n",
    "- In terms of test set, the regressor correctly predicts the quality rating 414 times. Meanwhile, the regressor overestimates (or underestimates) the quality rating by one (or minus one) 138 times (or 150 times). Even worse, it overestimates the quality rating by 3 one times.\n",
    "- For the training set, the regressor performs well. It correctly predicts the quality rating with 2583 times, and overestimates (or underestimates) the quality rating by one (or minus one) 686 times (or 791 times).\n",
    "- Thus, there are some problems with adaboost regressor that it is easy to overestimate or underestimate values even though estimating the quality rating by one or minus one seems not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(max_depth=2, n_estimators=6, learning_rate=1, random_state=42)\n",
    "gbr.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_prd_test = gbr.predict(X_test_sc)\n",
    "gbr_prd_train = gbr.predict(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_gbr_prd_test = np.around(gbr_prd_test)\n",
    "int_gbr_prd_train = np.around(gbr_prd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a GradientBoostingRegressor with only 6 estimators. There are same processes transforming predictive values into integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Inspect Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mse_test = mean_squared_error(y_test, int_gbr_prd_test)\n",
    "print(\"Mean square error of final model in test data is %s \" % gbr_mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_prd_value_irt = gbr.staged_predict(X_test_sc)\n",
    "gbr_mes_irt = []\n",
    "for i in gbr_prd_value_irt:\n",
    "    int_gbr_prd_value_irt = np.around(i)\n",
    "    gbr_mes_irt.append(mean_squared_error(y_test, int_gbr_prd_value_irt))\n",
    "print(\"The MSEs of all the models in test set \\n\")\n",
    "print(gbr_mes_irt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_mse_train = mean_squared_error(y_train, int_gbr_prd_train)\n",
    "print(\"Mean square error of final model in training data is %s \" % gbr_mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_prd_value_irt2 = gbr.staged_predict(X_train_sc)\n",
    "gbr_mes_irt2 = []\n",
    "for i in gbr_prd_value_irt2:\n",
    "    int_gbr_prd_value_irt2 = np.around(i)\n",
    "    gbr_mes_irt2.append(mean_squared_error(y_train, int_gbr_prd_value_irt2))\n",
    "print(\"The MSEs of all the models in train set \\n\")\n",
    "print(gbr_mes_irt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [1,2,3,4,5,6]\n",
    "plt.plot(x, gbr_mes_irt, marker=\"o\", c=\"r\", label=\"MSE of test\")\n",
    "plt.plot(x, gbr_mes_irt2, marker=\"*\", c=\"b\", label=\"MSE of train\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iteration time\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared the mean square error of final models on traning set with test set, they are similar 0.68 and 0.60 respectively.\n",
    "- The MSEs of all the intermediate models are computed on both datasets and are exhibited by a line chart. we know the two lines decrease significantly as iteration time increases.\n",
    "- when iteration time is equal to 5, it seems better for test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  Inspect Raw Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_gbr_test = pd.DataFrame()\n",
    "errors_gbr_test['predictive'] = int_gbr_prd_test\n",
    "errors_gbr_test['y_test'] = y_test.tolist()\n",
    "errors_gbr_test['errors'] = errors_gbr_test['predictive'] - errors_gbr_test['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_test2 = errors_gbr_test['errors'].value_counts()\n",
    "cnt_test2 = cnt_test2.sort_index(ascending=True)\n",
    "cnt_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab = cnt_test2.index\n",
    "ss = np.arange(len(xlab))\n",
    "plt.bar(ss, cnt_test2)\n",
    "plt.xticks(ss, xlab)\n",
    "plt.ylabel('Count')\n",
    "plt.title('the raw errors of the final model for Gradient Boosting on test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_gbr_train = pd.DataFrame()\n",
    "errors_gbr_train['predictive'] = int_gbr_prd_train\n",
    "errors_gbr_train['y_test'] = y_train.tolist()\n",
    "errors_gbr_train['errors'] = errors_gbr_train['predictive'] - errors_gbr_train['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_train2 = errors_gbr_train['errors'].value_counts()\n",
    "cnt_train2 = cnt_train2.sort_index(ascending=True)\n",
    "cnt_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab = cnt_train2.index\n",
    "ss = np.arange(len(xlab))\n",
    "plt.bar(ss, cnt_train2)\n",
    "plt.xticks(ss, xlab)\n",
    "plt.ylabel('Count')\n",
    "plt.title('the raw errors of the final model for Gradient Boosting on train set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In terms of test set, the regressor correctly predicts the quality rating 360 times. Meanwhile, the regressor overestimates (or underestimates) the quality rating by one (or minus one) 160 times (or 180 times). However, it overestimates the quality rating by 3 four times.\n",
    "- For the training set, the regressor performs not good. It correctly predicts the quality rating with only 2222 times, and overestimates (or underestimates) the quality rating by one (or minus one) 842 times (or 942 times). There are 12 samples which are wrongly predicted the quality rating by 3,(-3) or 4.\n",
    "- Thus, the Gradient Boosting regressor performs not well in this case both on training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Compare MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6]\n",
    "plt.plot(x, mes_irt, marker=\"o\", c='r', label=\"Ada test\")\n",
    "plt.plot(x, mes_irt2, marker=\"o\", c='b', label=\"Ada train\")\n",
    "plt.plot(x, gbr_mes_irt, marker=\"*\", c=\"y\", label=\"gbr test\")\n",
    "plt.plot(x, gbr_mes_irt2, marker=\"*\", c=\"g\", label=\"gbr train\")\n",
    "plt.legend(prop={'size': 7.5})\n",
    "plt.xlabel(\"iteration time\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MSEs are put in a plot. The AdaBoost regressor fits the data well that it is lower in MSE values for both training and test set. In contrast, the gradientBoost regressor is underfitting, the MSE is slightly higher than previous model. But the gap between training set and test set is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Compare Raw Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- If we assume that the quality rating by one or minus one is not bad. The adaboost regressor did a good job on trainin set, but there are **23** samples overestimated or underestimated over 1 on test set.\n",
    "- Under the same assumptions, for GradientBoosting regressor, it performed not well on the test set because of underfitting. Besides, there are only **360** samples predicted correctly (**414** in previous model) and it also predicted badly **35** samples.\n",
    "- In conclusion, Adaboost performed well but GradientBoosting is underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
